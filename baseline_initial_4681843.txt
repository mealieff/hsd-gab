== Directory: sing_label_data ==

== Training with: sing_label_data/multiclass_ADASYN_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.617
Recall:    0.615
F1 Score:  0.612

Microaveraged Scores per Label:
HD: Precision=0.203 Recall=0.609 F1=0.304
CV: Precision=0.043 Recall=0.208 F1=0.072
VO: Precision=0.176 Recall=0.591 F1=0.271
NONE: Precision=0.968 Recall=0.616 F1=0.753

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_ADASYN_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.670
Recall:    0.660
F1 Score:  0.662

Microaveraged Scores per Label:
HD: Precision=0.218 Recall=0.513 F1=0.306
CV: Precision=0.050 Recall=0.208 F1=0.081
VO: Precision=0.198 Recall=0.518 F1=0.287
NONE: Precision=0.960 Recall=0.681 F1=0.797

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_ADASYN_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.719
Recall:    0.706
F1 Score:  0.710

Microaveraged Scores per Label:
HD: Precision=0.237 Recall=0.442 F1=0.308
CV: Precision=0.046 Recall=0.167 F1=0.072
VO: Precision=0.222 Recall=0.450 F1=0.297
NONE: Precision=0.951 Recall=0.744 F1=0.835

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_ADASYN_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.766
Recall:    0.756
F1 Score:  0.759

Microaveraged Scores per Label:
HD: Precision=0.261 Recall=0.377 F1=0.309
CV: Precision=0.051 Recall=0.167 F1=0.078
VO: Precision=0.256 Recall=0.388 F1=0.308
NONE: Precision=0.934 Recall=0.810 F1=0.868

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_ADASYN_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.806
Recall:    0.797
F1 Score:  0.800

Microaveraged Scores per Label:
HD: Precision=0.303 Recall=0.320 F1=0.311
CV: Precision=0.060 Recall=0.167 F1=0.088
VO: Precision=0.293 Recall=0.322 F1=0.307
NONE: Precision=0.921 Recall=0.867 F1=0.893

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_ADASYN_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.830
Recall:    0.824
F1 Score:  0.826

Microaveraged Scores per Label:
HD: Precision=0.311 Recall=0.214 F1=0.253
CV: Precision=0.077 Recall=0.167 F1=0.105
VO: Precision=0.298 Recall=0.214 F1=0.249
NONE: Precision=0.903 Recall=0.913 F1=0.908

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.635
Recall:    0.631
F1 Score:  0.631

Microaveraged Scores per Label:
HD: Precision=0.204 Recall=0.466 F1=0.284
CV: Precision=0.029 Recall=0.083 F1=0.043
VO: Precision=0.147 Recall=0.499 F1=0.227
NONE: Precision=0.936 Recall=0.656 F1=0.771

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.714
Recall:    0.705
F1 Score:  0.708

Microaveraged Scores per Label:
HD: Precision=0.219 Recall=0.340 F1=0.267
CV: Precision=0.067 Recall=0.083 F1=0.074
VO: Precision=0.174 Recall=0.388 F1=0.240
NONE: Precision=0.921 Recall=0.758 F1=0.832

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.778
Recall:    0.769
F1 Score:  0.772

Microaveraged Scores per Label:
HD: Precision=0.245 Recall=0.240 F1=0.243
CV: Precision=0.154 Recall=0.083 F1=0.108
VO: Precision=0.196 Recall=0.287 F1=0.233
NONE: Precision=0.907 Recall=0.845 F1=0.875

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.819
Recall:    0.814
F1 Score:  0.815

Microaveraged Scores per Label:
HD: Precision=0.263 Recall=0.157 F1=0.196
CV: Precision=0.400 Recall=0.083 F1=0.138
VO: Precision=0.224 Recall=0.201 F1=0.211
NONE: Precision=0.894 Recall=0.908 F1=0.901

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.841
Recall:    0.837
F1 Score:  0.838

Microaveraged Scores per Label:
HD: Precision=0.275 Recall=0.096 F1=0.142
CV: Precision=0.333 Recall=0.042 F1=0.074
VO: Precision=0.215 Recall=0.111 F1=0.146
NONE: Precision=0.883 Recall=0.946 F1=0.914

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.859
Recall:    0.858
F1 Score:  0.858

Microaveraged Scores per Label:
HD: Precision=0.286 Recall=0.045 F1=0.077
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.250 Recall=0.057 F1=0.093
NONE: Precision=0.877 Recall=0.977 F1=0.924

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.567
Recall:    0.567
F1 Score:  0.564

Microaveraged Scores per Label:
HD: Precision=0.191 Recall=0.672 F1=0.297
CV: Precision=0.044 Recall=0.208 F1=0.073
VO: Precision=0.158 Recall=0.615 F1=0.251
NONE: Precision=0.971 Recall=0.555 F1=0.707

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.633
Recall:    0.625
F1 Score:  0.626

Microaveraged Scores per Label:
HD: Precision=0.213 Recall=0.585 F1=0.312
CV: Precision=0.051 Recall=0.208 F1=0.081
VO: Precision=0.186 Recall=0.542 F1=0.277
NONE: Precision=0.964 Recall=0.633 F1=0.764

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.704
Recall:    0.691
F1 Score:  0.695

Microaveraged Scores per Label:
HD: Precision=0.239 Recall=0.491 F1=0.322
CV: Precision=0.057 Recall=0.208 F1=0.089
VO: Precision=0.214 Recall=0.450 F1=0.290
NONE: Precision=0.954 Recall=0.722 F1=0.822

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.759
Recall:    0.749
F1 Score:  0.752

Microaveraged Scores per Label:
HD: Precision=0.254 Recall=0.379 F1=0.304
CV: Precision=0.063 Recall=0.208 F1=0.097
VO: Precision=0.245 Recall=0.369 F1=0.294
NONE: Precision=0.930 Recall=0.803 F1=0.862

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.805
Recall:    0.797
F1 Score:  0.800

Microaveraged Scores per Label:
HD: Precision=0.282 Recall=0.271 F1=0.276
CV: Precision=0.062 Recall=0.167 F1=0.090
VO: Precision=0.280 Recall=0.285 F1=0.282
NONE: Precision=0.912 Recall=0.873 F1=0.892

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.836
Recall:    0.832
F1 Score:  0.833

Microaveraged Scores per Label:
HD: Precision=0.323 Recall=0.175 F1=0.227
CV: Precision=0.058 Recall=0.125 F1=0.079
VO: Precision=0.279 Recall=0.154 F1=0.199
NONE: Precision=0.895 Recall=0.929 F1=0.912

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.298
Recall:    0.300
F1 Score:  0.295

Microaveraged Scores per Label:
HD: Precision=0.151 Recall=0.654 F1=0.246
CV: Precision=0.012 Recall=0.583 F1=0.023
VO: Precision=0.105 Recall=0.564 F1=0.176
NONE: Precision=0.981 Recall=0.252 F1=0.401

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['CV']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.417
Recall:    0.408
F1 Score:  0.410

Microaveraged Scores per Label:
HD: Precision=0.167 Recall=0.532 F1=0.254
CV: Precision=0.015 Recall=0.458 F1=0.030
VO: Precision=0.117 Recall=0.453 F1=0.186
NONE: Precision=0.956 Recall=0.392 F1=0.557

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['CV']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.556
Recall:    0.546
F1 Score:  0.549

Microaveraged Scores per Label:
HD: Precision=0.173 Recall=0.403 F1=0.242
CV: Precision=0.020 Recall=0.333 F1=0.038
VO: Precision=0.113 Recall=0.293 F1=0.163
NONE: Precision=0.914 Recall=0.572 F1=0.704

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['CV']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.679
Recall:    0.672
F1 Score:  0.675

Microaveraged Scores per Label:
HD: Precision=0.189 Recall=0.297 F1=0.231
CV: Precision=0.028 Recall=0.250 F1=0.051
VO: Precision=0.126 Recall=0.203 F1=0.155
NONE: Precision=0.895 Recall=0.732 F1=0.805

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['CV']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.768
Recall:    0.764
F1 Score:  0.765

Microaveraged Scores per Label:
HD: Precision=0.216 Recall=0.224 F1=0.220
CV: Precision=0.051 Recall=0.208 F1=0.082
VO: Precision=0.130 Recall=0.111 F1=0.120
NONE: Precision=0.888 Recall=0.848 F1=0.868

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.821
Recall:    0.819
F1 Score:  0.819

Microaveraged Scores per Label:
HD: Precision=0.215 Recall=0.126 F1=0.159
CV: Precision=0.061 Recall=0.083 F1=0.070
VO: Precision=0.123 Recall=0.046 F1=0.067
NONE: Precision=0.879 Recall=0.925 F1=0.901

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTEENN_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.411
Recall:    0.410
F1 Score:  0.407

Microaveraged Scores per Label:
HD: Precision=0.150 Recall=0.690 F1=0.246
CV: Precision=0.024 Recall=0.208 F1=0.043
VO: Precision=0.117 Recall=0.629 F1=0.197
NONE: Precision=0.986 Recall=0.372 F1=0.540

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTEENN_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.440
Recall:    0.433
F1 Score:  0.434

Microaveraged Scores per Label:
HD: Precision=0.157 Recall=0.633 F1=0.252
CV: Precision=0.022 Recall=0.167 F1=0.040
VO: Precision=0.126 Recall=0.585 F1=0.207
NONE: Precision=0.985 Recall=0.405 F1=0.574

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTEENN_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.472
Recall:    0.458
F1 Score:  0.463

Microaveraged Scores per Label:
HD: Precision=0.157 Recall=0.554 F1=0.244
CV: Precision=0.026 Recall=0.167 F1=0.045
VO: Precision=0.135 Recall=0.531 F1=0.215
NONE: Precision=0.982 Recall=0.444 F1=0.611

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTEENN_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.532
Recall:    0.520
F1 Score:  0.524

Microaveraged Scores per Label:
HD: Precision=0.163 Recall=0.499 F1=0.245
CV: Precision=0.022 Recall=0.125 F1=0.038
VO: Precision=0.137 Recall=0.450 F1=0.210
NONE: Precision=0.949 Recall=0.525 F1=0.676

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTEENN_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.596
Recall:    0.585
F1 Score:  0.589

Microaveraged Scores per Label:
HD: Precision=0.170 Recall=0.436 F1=0.245
CV: Precision=0.028 Recall=0.125 F1=0.045
VO: Precision=0.144 Recall=0.393 F1=0.211
NONE: Precision=0.931 Recall=0.608 F1=0.736

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTEENN_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.663
Recall:    0.654
F1 Score:  0.657

Microaveraged Scores per Label:
HD: Precision=0.173 Recall=0.344 F1=0.231
CV: Precision=0.032 Recall=0.125 F1=0.050
VO: Precision=0.157 Recall=0.320 F1=0.210
NONE: Precision=0.912 Recall=0.700 F1=0.792

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTE_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.621
Recall:    0.618
F1 Score:  0.616

Microaveraged Scores per Label:
HD: Precision=0.204 Recall=0.603 F1=0.305
CV: Precision=0.040 Recall=0.208 F1=0.068
VO: Precision=0.178 Recall=0.583 F1=0.273
NONE: Precision=0.968 Recall=0.621 F1=0.756

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTE_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.675
Recall:    0.666
F1 Score:  0.668

Microaveraged Scores per Label:
HD: Precision=0.225 Recall=0.525 F1=0.315
CV: Precision=0.039 Recall=0.167 F1=0.063
VO: Precision=0.206 Recall=0.528 F1=0.296
NONE: Precision=0.962 Recall=0.686 F1=0.801

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTE_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.722
Recall:    0.709
F1 Score:  0.714

Microaveraged Scores per Label:
HD: Precision=0.242 Recall=0.456 F1=0.316
CV: Precision=0.043 Recall=0.167 F1=0.068
VO: Precision=0.225 Recall=0.444 F1=0.299
NONE: Precision=0.953 Recall=0.747 F1=0.838

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTE_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.769
Recall:    0.758
F1 Score:  0.762

Microaveraged Scores per Label:
HD: Precision=0.262 Recall=0.379 F1=0.310
CV: Precision=0.051 Recall=0.167 F1=0.078
VO: Precision=0.263 Recall=0.398 F1=0.317
NONE: Precision=0.937 Recall=0.812 F1=0.870

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTE_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.810
Recall:    0.800
F1 Score:  0.803

Microaveraged Scores per Label:
HD: Precision=0.307 Recall=0.314 F1=0.310
CV: Precision=0.057 Recall=0.167 F1=0.085
VO: Precision=0.295 Recall=0.317 F1=0.306
NONE: Precision=0.922 Recall=0.872 F1=0.896

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_SMOTE_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.833
Recall:    0.828
F1 Score:  0.829

Microaveraged Scores per Label:
HD: Precision=0.319 Recall=0.216 F1=0.258
CV: Precision=0.075 Recall=0.167 F1=0.104
VO: Precision=0.320 Recall=0.225 F1=0.264
NONE: Precision=0.903 Recall=0.915 F1=0.909

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_TomekLinks_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.873
Recall:    0.868
F1 Score:  0.869

Microaveraged Scores per Label:
HD: Precision=0.469 Recall=0.358 F1=0.406
CV: Precision=0.250 Recall=0.083 F1=0.125
VO: Precision=0.483 Recall=0.301 F1=0.371
NONE: Precision=0.917 Recall=0.949 F1=0.933

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_TomekLinks_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.881
Recall:    0.874
F1 Score:  0.877

Microaveraged Scores per Label:
HD: Precision=0.513 Recall=0.242 F1=0.329
CV: Precision=0.286 Recall=0.083 F1=0.129
VO: Precision=0.596 Recall=0.228 F1=0.329
NONE: Precision=0.906 Recall=0.970 F1=0.937

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_TomekLinks_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.881
Recall:    0.875
F1 Score:  0.877

Microaveraged Scores per Label:
HD: Precision=0.556 Recall=0.153 F1=0.240
CV: Precision=0.333 Recall=0.083 F1=0.133
VO: Precision=0.629 Recall=0.152 F1=0.245
NONE: Precision=0.894 Recall=0.983 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_TomekLinks_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.878
Recall:    0.875
F1 Score:  0.876

Microaveraged Scores per Label:
HD: Precision=0.620 Recall=0.090 F1=0.157
CV: Precision=0.667 Recall=0.083 F1=0.148
VO: Precision=0.688 Recall=0.089 F1=0.158
NONE: Precision=0.884 Recall=0.991 F1=0.934

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_TomekLinks_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.875
Recall:    0.873
F1 Score:  0.874

Microaveraged Scores per Label:
HD: Precision=0.655 Recall=0.039 F1=0.073
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.625 Recall=0.054 F1=0.100
NONE: Precision=0.877 Recall=0.995 F1=0.933

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data/multiclass_TomekLinks_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.873
Recall:    0.873
F1 Score:  0.873

Microaveraged Scores per Label:
HD: Precision=0.571 Recall=0.008 F1=0.016
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.722 Recall=0.035 F1=0.067
NONE: Precision=0.874 Recall=0.998 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.
== Directory: sing_label_data2_1 ==

== Training with: sing_label_data2_1/multiclass_ADASYN_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.617
Recall:    0.615
F1 Score:  0.612

Microaveraged Scores per Label:
HD: Precision=0.203 Recall=0.609 F1=0.304
CV: Precision=0.043 Recall=0.208 F1=0.072
VO: Precision=0.176 Recall=0.591 F1=0.271
NONE: Precision=0.968 Recall=0.616 F1=0.753

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_ADASYN_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.670
Recall:    0.660
F1 Score:  0.662

Microaveraged Scores per Label:
HD: Precision=0.218 Recall=0.513 F1=0.306
CV: Precision=0.050 Recall=0.208 F1=0.081
VO: Precision=0.198 Recall=0.518 F1=0.287
NONE: Precision=0.960 Recall=0.681 F1=0.797

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_ADASYN_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.719
Recall:    0.706
F1 Score:  0.710

Microaveraged Scores per Label:
HD: Precision=0.237 Recall=0.442 F1=0.308
CV: Precision=0.046 Recall=0.167 F1=0.072
VO: Precision=0.222 Recall=0.450 F1=0.297
NONE: Precision=0.951 Recall=0.744 F1=0.835

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_ADASYN_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.766
Recall:    0.756
F1 Score:  0.759

Microaveraged Scores per Label:
HD: Precision=0.261 Recall=0.377 F1=0.309
CV: Precision=0.051 Recall=0.167 F1=0.078
VO: Precision=0.256 Recall=0.388 F1=0.308
NONE: Precision=0.934 Recall=0.810 F1=0.868

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_ADASYN_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.806
Recall:    0.797
F1 Score:  0.800

Microaveraged Scores per Label:
HD: Precision=0.303 Recall=0.320 F1=0.311
CV: Precision=0.060 Recall=0.167 F1=0.088
VO: Precision=0.293 Recall=0.322 F1=0.307
NONE: Precision=0.921 Recall=0.867 F1=0.893

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_ADASYN_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.830
Recall:    0.824
F1 Score:  0.826

Microaveraged Scores per Label:
HD: Precision=0.311 Recall=0.214 F1=0.253
CV: Precision=0.077 Recall=0.167 F1=0.105
VO: Precision=0.298 Recall=0.214 F1=0.249
NONE: Precision=0.903 Recall=0.913 F1=0.908

Example Confidence Scores:
Test sample 0:
  HD: 0.311
  CV: 0.000
  VO: 0.055
  NONE: 0.634
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.235
  CV: 0.000
  VO: 0.052
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.449
  CV: 0.000
  VO: 0.077
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.029
  CV: 0.000
  VO: 0.000
  NONE: 0.970
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.858
  CV: 0.006
  VO: 0.093
  NONE: 0.042
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.801
  CV: 0.000
  VO: 0.192
  NONE: 0.007
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.030
  CV: 0.000
  VO: 0.028
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.198
  NONE: 0.755
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.002
  CV: 0.000
  VO: 0.005
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.367
  CV: 0.000
  VO: 0.047
  NONE: 0.585
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.133
  CV: 0.000
  VO: 0.003
  NONE: 0.864
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.031
  CV: 0.001
  VO: 0.962
  NONE: 0.005
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.076
  CV: 0.000
  VO: 0.008
  NONE: 0.916
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.203
  CV: 0.050
  VO: 0.747
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.635
Recall:    0.631
F1 Score:  0.631

Microaveraged Scores per Label:
HD: Precision=0.204 Recall=0.466 F1=0.284
CV: Precision=0.029 Recall=0.083 F1=0.043
VO: Precision=0.147 Recall=0.499 F1=0.227
NONE: Precision=0.936 Recall=0.656 F1=0.771

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.714
Recall:    0.705
F1 Score:  0.708

Microaveraged Scores per Label:
HD: Precision=0.219 Recall=0.340 F1=0.267
CV: Precision=0.067 Recall=0.083 F1=0.074
VO: Precision=0.174 Recall=0.388 F1=0.240
NONE: Precision=0.921 Recall=0.758 F1=0.832

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.778
Recall:    0.769
F1 Score:  0.772

Microaveraged Scores per Label:
HD: Precision=0.245 Recall=0.240 F1=0.243
CV: Precision=0.154 Recall=0.083 F1=0.108
VO: Precision=0.196 Recall=0.287 F1=0.233
NONE: Precision=0.907 Recall=0.845 F1=0.875

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.819
Recall:    0.814
F1 Score:  0.815

Microaveraged Scores per Label:
HD: Precision=0.263 Recall=0.157 F1=0.196
CV: Precision=0.400 Recall=0.083 F1=0.138
VO: Precision=0.224 Recall=0.201 F1=0.211
NONE: Precision=0.894 Recall=0.908 F1=0.901

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.841
Recall:    0.837
F1 Score:  0.838

Microaveraged Scores per Label:
HD: Precision=0.275 Recall=0.096 F1=0.142
CV: Precision=0.333 Recall=0.042 F1=0.074
VO: Precision=0.215 Recall=0.111 F1=0.146
NONE: Precision=0.883 Recall=0.946 F1=0.914

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_CondensedNearestNeighbour_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.859
Recall:    0.858
F1 Score:  0.858

Microaveraged Scores per Label:
HD: Precision=0.286 Recall=0.045 F1=0.077
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.250 Recall=0.057 F1=0.093
NONE: Precision=0.877 Recall=0.977 F1=0.924

Example Confidence Scores:
Test sample 0:
  HD: 0.022
  CV: 0.128
  VO: 0.505
  NONE: 0.345
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.581
  CV: 0.000
  VO: 0.293
  NONE: 0.125
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.133
  CV: 0.010
  VO: 0.855
  NONE: 0.003
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 3:
  HD: 0.162
  CV: 0.008
  VO: 0.133
  NONE: 0.697
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.067
  CV: 0.032
  VO: 0.088
  NONE: 0.813
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.028
  CV: 0.003
  VO: 0.908
  NONE: 0.061
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.096
  CV: 0.006
  VO: 0.310
  NONE: 0.588
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.010
  CV: 0.025
  VO: 0.482
  NONE: 0.483
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.029
  CV: 0.001
  VO: 0.009
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.314
  CV: 0.062
  VO: 0.096
  NONE: 0.528
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.027
  CV: 0.001
  VO: 0.008
  NONE: 0.965
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.132
  CV: 0.012
  VO: 0.122
  NONE: 0.734
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.040
  CV: 0.210
  VO: 0.275
  NONE: 0.475
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.228
  CV: 0.110
  VO: 0.513
  NONE: 0.149
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.567
Recall:    0.567
F1 Score:  0.564

Microaveraged Scores per Label:
HD: Precision=0.191 Recall=0.672 F1=0.297
CV: Precision=0.044 Recall=0.208 F1=0.073
VO: Precision=0.158 Recall=0.615 F1=0.251
NONE: Precision=0.971 Recall=0.555 F1=0.707

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.633
Recall:    0.625
F1 Score:  0.626

Microaveraged Scores per Label:
HD: Precision=0.213 Recall=0.585 F1=0.312
CV: Precision=0.051 Recall=0.208 F1=0.081
VO: Precision=0.186 Recall=0.542 F1=0.277
NONE: Precision=0.964 Recall=0.633 F1=0.764

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.704
Recall:    0.691
F1 Score:  0.695

Microaveraged Scores per Label:
HD: Precision=0.239 Recall=0.491 F1=0.322
CV: Precision=0.057 Recall=0.208 F1=0.089
VO: Precision=0.214 Recall=0.450 F1=0.290
NONE: Precision=0.954 Recall=0.722 F1=0.822

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.759
Recall:    0.749
F1 Score:  0.752

Microaveraged Scores per Label:
HD: Precision=0.254 Recall=0.379 F1=0.304
CV: Precision=0.063 Recall=0.208 F1=0.097
VO: Precision=0.245 Recall=0.369 F1=0.294
NONE: Precision=0.930 Recall=0.803 F1=0.862

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.805
Recall:    0.797
F1 Score:  0.800

Microaveraged Scores per Label:
HD: Precision=0.282 Recall=0.271 F1=0.276
CV: Precision=0.062 Recall=0.167 F1=0.090
VO: Precision=0.280 Recall=0.285 F1=0.282
NONE: Precision=0.912 Recall=0.873 F1=0.892

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.836
Recall:    0.832
F1 Score:  0.833

Microaveraged Scores per Label:
HD: Precision=0.323 Recall=0.175 F1=0.227
CV: Precision=0.058 Recall=0.125 F1=0.079
VO: Precision=0.279 Recall=0.154 F1=0.199
NONE: Precision=0.895 Recall=0.929 F1=0.912

Example Confidence Scores:
Test sample 0:
  HD: 0.386
  CV: 0.000
  VO: 0.032
  NONE: 0.582
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.238
  CV: 0.000
  VO: 0.049
  NONE: 0.713
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.405
  CV: 0.000
  VO: 0.092
  NONE: 0.502
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.013
  CV: 0.000
  VO: 0.001
  NONE: 0.986
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.849
  CV: 0.003
  VO: 0.092
  NONE: 0.055
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.754
  CV: 0.000
  VO: 0.242
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.067
  CV: 0.000
  VO: 0.033
  NONE: 0.900
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.109
  CV: 0.000
  VO: 0.481
  NONE: 0.410
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.011
  CV: 0.000
  VO: 0.008
  NONE: 0.981
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.241
  CV: 0.000
  VO: 0.090
  NONE: 0.668
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.145
  CV: 0.000
  VO: 0.011
  NONE: 0.843
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.001
  VO: 0.948
  NONE: 0.006
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.146
  CV: 0.000
  VO: 0.024
  NONE: 0.830
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.400
  CV: 0.035
  VO: 0.565
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.298
Recall:    0.300
F1 Score:  0.295

Microaveraged Scores per Label:
HD: Precision=0.151 Recall=0.654 F1=0.246
CV: Precision=0.012 Recall=0.583 F1=0.023
VO: Precision=0.105 Recall=0.564 F1=0.176
NONE: Precision=0.981 Recall=0.252 F1=0.401

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['CV']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.417
Recall:    0.408
F1 Score:  0.410

Microaveraged Scores per Label:
HD: Precision=0.167 Recall=0.532 F1=0.254
CV: Precision=0.015 Recall=0.458 F1=0.030
VO: Precision=0.117 Recall=0.453 F1=0.186
NONE: Precision=0.956 Recall=0.392 F1=0.557

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['CV']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.556
Recall:    0.546
F1 Score:  0.549

Microaveraged Scores per Label:
HD: Precision=0.173 Recall=0.403 F1=0.242
CV: Precision=0.020 Recall=0.333 F1=0.038
VO: Precision=0.113 Recall=0.293 F1=0.163
NONE: Precision=0.914 Recall=0.572 F1=0.704

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['CV']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.679
Recall:    0.672
F1 Score:  0.675

Microaveraged Scores per Label:
HD: Precision=0.189 Recall=0.297 F1=0.231
CV: Precision=0.028 Recall=0.250 F1=0.051
VO: Precision=0.126 Recall=0.203 F1=0.155
NONE: Precision=0.895 Recall=0.732 F1=0.805

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['CV']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.768
Recall:    0.764
F1 Score:  0.765

Microaveraged Scores per Label:
HD: Precision=0.216 Recall=0.224 F1=0.220
CV: Precision=0.051 Recall=0.208 F1=0.082
VO: Precision=0.130 Recall=0.111 F1=0.120
NONE: Precision=0.888 Recall=0.848 F1=0.868

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.821
Recall:    0.819
F1 Score:  0.819

Microaveraged Scores per Label:
HD: Precision=0.215 Recall=0.126 F1=0.159
CV: Precision=0.061 Recall=0.083 F1=0.070
VO: Precision=0.123 Recall=0.046 F1=0.067
NONE: Precision=0.879 Recall=0.925 F1=0.901

Example Confidence Scores:
Test sample 0:
  HD: 0.035
  CV: 0.053
  VO: 0.018
  NONE: 0.894
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.673
  CV: 0.030
  VO: 0.102
  NONE: 0.195
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.062
  CV: 0.046
  VO: 0.099
  NONE: 0.793
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.239
  CV: 0.186
  VO: 0.052
  NONE: 0.522
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.243
  CV: 0.610
  VO: 0.117
  NONE: 0.030
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.277
  CV: 0.067
  VO: 0.609
  NONE: 0.047
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.331
  CV: 0.084
  VO: 0.087
  NONE: 0.498
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.031
  CV: 0.190
  VO: 0.432
  NONE: 0.347
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.449
  CV: 0.135
  VO: 0.051
  NONE: 0.365
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.718
  CV: 0.161
  VO: 0.088
  NONE: 0.033
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.308
  CV: 0.263
  VO: 0.222
  NONE: 0.207
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.263
  CV: 0.150
  VO: 0.155
  NONE: 0.432
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.121
  CV: 0.178
  VO: 0.597
  NONE: 0.104
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.430
  CV: 0.060
  VO: 0.508
  NONE: 0.002
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.411
Recall:    0.410
F1 Score:  0.407

Microaveraged Scores per Label:
HD: Precision=0.150 Recall=0.690 F1=0.246
CV: Precision=0.024 Recall=0.208 F1=0.043
VO: Precision=0.117 Recall=0.629 F1=0.197
NONE: Precision=0.986 Recall=0.372 F1=0.540

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.440
Recall:    0.433
F1 Score:  0.434

Microaveraged Scores per Label:
HD: Precision=0.157 Recall=0.633 F1=0.252
CV: Precision=0.022 Recall=0.167 F1=0.040
VO: Precision=0.126 Recall=0.585 F1=0.207
NONE: Precision=0.985 Recall=0.405 F1=0.574

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.472
Recall:    0.458
F1 Score:  0.463

Microaveraged Scores per Label:
HD: Precision=0.157 Recall=0.554 F1=0.244
CV: Precision=0.026 Recall=0.167 F1=0.045
VO: Precision=0.135 Recall=0.531 F1=0.215
NONE: Precision=0.982 Recall=0.444 F1=0.611

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.532
Recall:    0.520
F1 Score:  0.524

Microaveraged Scores per Label:
HD: Precision=0.163 Recall=0.499 F1=0.245
CV: Precision=0.022 Recall=0.125 F1=0.038
VO: Precision=0.137 Recall=0.450 F1=0.210
NONE: Precision=0.949 Recall=0.525 F1=0.676

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.596
Recall:    0.585
F1 Score:  0.589

Microaveraged Scores per Label:
HD: Precision=0.170 Recall=0.436 F1=0.245
CV: Precision=0.028 Recall=0.125 F1=0.045
VO: Precision=0.144 Recall=0.393 F1=0.211
NONE: Precision=0.931 Recall=0.608 F1=0.736

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.663
Recall:    0.654
F1 Score:  0.657

Microaveraged Scores per Label:
HD: Precision=0.173 Recall=0.344 F1=0.231
CV: Precision=0.032 Recall=0.125 F1=0.050
VO: Precision=0.157 Recall=0.320 F1=0.210
NONE: Precision=0.912 Recall=0.700 F1=0.792

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTE_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.621
Recall:    0.618
F1 Score:  0.616

Microaveraged Scores per Label:
HD: Precision=0.204 Recall=0.603 F1=0.305
CV: Precision=0.040 Recall=0.208 F1=0.068
VO: Precision=0.178 Recall=0.583 F1=0.273
NONE: Precision=0.968 Recall=0.621 F1=0.756

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTE_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.675
Recall:    0.666
F1 Score:  0.668

Microaveraged Scores per Label:
HD: Precision=0.225 Recall=0.525 F1=0.315
CV: Precision=0.039 Recall=0.167 F1=0.063
VO: Precision=0.206 Recall=0.528 F1=0.296
NONE: Precision=0.962 Recall=0.686 F1=0.801

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTE_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.722
Recall:    0.709
F1 Score:  0.714

Microaveraged Scores per Label:
HD: Precision=0.242 Recall=0.456 F1=0.316
CV: Precision=0.043 Recall=0.167 F1=0.068
VO: Precision=0.225 Recall=0.444 F1=0.299
NONE: Precision=0.953 Recall=0.747 F1=0.838

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTE_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.769
Recall:    0.758
F1 Score:  0.762

Microaveraged Scores per Label:
HD: Precision=0.262 Recall=0.379 F1=0.310
CV: Precision=0.051 Recall=0.167 F1=0.078
VO: Precision=0.263 Recall=0.398 F1=0.317
NONE: Precision=0.937 Recall=0.812 F1=0.870

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTE_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.810
Recall:    0.800
F1 Score:  0.803

Microaveraged Scores per Label:
HD: Precision=0.307 Recall=0.314 F1=0.310
CV: Precision=0.057 Recall=0.167 F1=0.085
VO: Precision=0.295 Recall=0.317 F1=0.306
NONE: Precision=0.922 Recall=0.872 F1=0.896

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_SMOTE_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.833
Recall:    0.828
F1 Score:  0.829

Microaveraged Scores per Label:
HD: Precision=0.319 Recall=0.216 F1=0.258
CV: Precision=0.075 Recall=0.167 F1=0.104
VO: Precision=0.320 Recall=0.225 F1=0.264
NONE: Precision=0.903 Recall=0.915 F1=0.909

Example Confidence Scores:
Test sample 0:
  HD: 0.305
  CV: 0.000
  VO: 0.032
  NONE: 0.663
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.160
  CV: 0.000
  VO: 0.052
  NONE: 0.787
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.453
  CV: 0.000
  VO: 0.080
  NONE: 0.467
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.022
  CV: 0.000
  VO: 0.000
  NONE: 0.978
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.870
  CV: 0.009
  VO: 0.076
  NONE: 0.045
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.745
  CV: 0.000
  VO: 0.244
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.027
  CV: 0.000
  VO: 0.029
  NONE: 0.944
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.047
  CV: 0.000
  VO: 0.145
  NONE: 0.808
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.003
  CV: 0.000
  VO: 0.006
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.393
  CV: 0.000
  VO: 0.056
  NONE: 0.550
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.088
  CV: 0.000
  VO: 0.002
  NONE: 0.910
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.044
  CV: 0.002
  VO: 0.948
  NONE: 0.007
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.070
  CV: 0.000
  VO: 0.006
  NONE: 0.924
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.246
  CV: 0.052
  VO: 0.702
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_TomekLinks_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.873
Recall:    0.868
F1 Score:  0.869

Microaveraged Scores per Label:
HD: Precision=0.469 Recall=0.358 F1=0.406
CV: Precision=0.250 Recall=0.083 F1=0.125
VO: Precision=0.483 Recall=0.301 F1=0.371
NONE: Precision=0.917 Recall=0.949 F1=0.933

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_TomekLinks_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.881
Recall:    0.874
F1 Score:  0.877

Microaveraged Scores per Label:
HD: Precision=0.513 Recall=0.242 F1=0.329
CV: Precision=0.286 Recall=0.083 F1=0.129
VO: Precision=0.596 Recall=0.228 F1=0.329
NONE: Precision=0.906 Recall=0.970 F1=0.937

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_TomekLinks_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.881
Recall:    0.875
F1 Score:  0.877

Microaveraged Scores per Label:
HD: Precision=0.556 Recall=0.153 F1=0.240
CV: Precision=0.333 Recall=0.083 F1=0.133
VO: Precision=0.629 Recall=0.152 F1=0.245
NONE: Precision=0.894 Recall=0.983 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_TomekLinks_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.878
Recall:    0.875
F1 Score:  0.876

Microaveraged Scores per Label:
HD: Precision=0.620 Recall=0.090 F1=0.157
CV: Precision=0.667 Recall=0.083 F1=0.148
VO: Precision=0.688 Recall=0.089 F1=0.158
NONE: Precision=0.884 Recall=0.991 F1=0.934

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_TomekLinks_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.875
Recall:    0.873
F1 Score:  0.874

Microaveraged Scores per Label:
HD: Precision=0.655 Recall=0.039 F1=0.073
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.625 Recall=0.054 F1=0.100
NONE: Precision=0.877 Recall=0.995 F1=0.933

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data2_1/multiclass_TomekLinks_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.873
Recall:    0.873
F1 Score:  0.873

Microaveraged Scores per Label:
HD: Precision=0.571 Recall=0.008 F1=0.016
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.722 Recall=0.035 F1=0.067
NONE: Precision=0.874 Recall=0.998 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.029
  CV: 0.001
  VO: 0.004
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.029
  CV: 0.000
  VO: 0.010
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.038
  CV: 0.000
  VO: 0.031
  NONE: 0.931
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.144
  CV: 0.011
  VO: 0.027
  NONE: 0.817
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.163
  CV: 0.000
  VO: 0.398
  NONE: 0.439
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.014
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.008
  CV: 0.000
  VO: 0.007
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.004
  CV: 0.000
  VO: 0.003
  NONE: 0.992
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.046
  CV: 0.000
  VO: 0.006
  NONE: 0.947
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.993
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.068
  CV: 0.003
  VO: 0.156
  NONE: 0.773
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.037
  CV: 0.014
  VO: 0.006
  NONE: 0.942
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.389
  CV: 0.016
  VO: 0.321
  NONE: 0.274
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.
== Directory: sing_label_data3_1 ==

== Training with: sing_label_data3_1/multiclass_ADASYN_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.869
Recall:    0.865
F1 Score:  0.866

Microaveraged Scores per Label:
HD: Precision=0.454 Recall=0.371 F1=0.408
CV: Precision=0.250 Recall=0.125 F1=0.167
VO: Precision=0.490 Recall=0.339 F1=0.401
NONE: Precision=0.920 Recall=0.943 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_ADASYN_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.880
Recall:    0.874
F1 Score:  0.876

Microaveraged Scores per Label:
HD: Precision=0.523 Recall=0.259 F1=0.346
CV: Precision=0.250 Recall=0.083 F1=0.125
VO: Precision=0.573 Recall=0.233 F1=0.331
NONE: Precision=0.907 Recall=0.968 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_ADASYN_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.882
Recall:    0.876
F1 Score:  0.878

Microaveraged Scores per Label:
HD: Precision=0.564 Recall=0.161 F1=0.250
CV: Precision=0.400 Recall=0.083 F1=0.138
VO: Precision=0.621 Recall=0.160 F1=0.254
NONE: Precision=0.895 Recall=0.982 F1=0.937

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_ADASYN_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.880
Recall:    0.877
F1 Score:  0.878

Microaveraged Scores per Label:
HD: Precision=0.638 Recall=0.090 F1=0.157
CV: Precision=0.500 Recall=0.083 F1=0.143
VO: Precision=0.706 Recall=0.098 F1=0.171
NONE: Precision=0.885 Recall=0.992 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_ADASYN_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.875
Recall:    0.874
F1 Score:  0.874

Microaveraged Scores per Label:
HD: Precision=0.600 Recall=0.031 F1=0.058
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.688 Recall=0.060 F1=0.110
NONE: Precision=0.877 Recall=0.996 F1=0.933

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_ADASYN_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.873
Recall:    0.872
F1 Score:  0.872

Microaveraged Scores per Label:
HD: Precision=0.667 Recall=0.008 F1=0.016
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.625 Recall=0.027 F1=0.052
NONE: Precision=0.874 Recall=0.998 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.869
Recall:    0.865
F1 Score:  0.866

Microaveraged Scores per Label:
HD: Precision=0.454 Recall=0.371 F1=0.408
CV: Precision=0.250 Recall=0.125 F1=0.167
VO: Precision=0.490 Recall=0.339 F1=0.401
NONE: Precision=0.920 Recall=0.943 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.880
Recall:    0.874
F1 Score:  0.876

Microaveraged Scores per Label:
HD: Precision=0.523 Recall=0.259 F1=0.346
CV: Precision=0.250 Recall=0.083 F1=0.125
VO: Precision=0.573 Recall=0.233 F1=0.331
NONE: Precision=0.907 Recall=0.968 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.882
Recall:    0.876
F1 Score:  0.878

Microaveraged Scores per Label:
HD: Precision=0.564 Recall=0.161 F1=0.250
CV: Precision=0.400 Recall=0.083 F1=0.138
VO: Precision=0.621 Recall=0.160 F1=0.254
NONE: Precision=0.895 Recall=0.982 F1=0.937

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.880
Recall:    0.877
F1 Score:  0.878

Microaveraged Scores per Label:
HD: Precision=0.638 Recall=0.090 F1=0.157
CV: Precision=0.500 Recall=0.083 F1=0.143
VO: Precision=0.706 Recall=0.098 F1=0.171
NONE: Precision=0.885 Recall=0.992 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.875
Recall:    0.874
F1 Score:  0.874

Microaveraged Scores per Label:
HD: Precision=0.600 Recall=0.031 F1=0.058
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.688 Recall=0.060 F1=0.110
NONE: Precision=0.877 Recall=0.996 F1=0.933

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomOverSampler_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.873
Recall:    0.872
F1 Score:  0.872

Microaveraged Scores per Label:
HD: Precision=0.667 Recall=0.008 F1=0.016
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.625 Recall=0.027 F1=0.052
NONE: Precision=0.874 Recall=0.998 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.869
Recall:    0.865
F1 Score:  0.866

Microaveraged Scores per Label:
HD: Precision=0.454 Recall=0.371 F1=0.408
CV: Precision=0.250 Recall=0.125 F1=0.167
VO: Precision=0.490 Recall=0.339 F1=0.401
NONE: Precision=0.920 Recall=0.943 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.880
Recall:    0.874
F1 Score:  0.876

Microaveraged Scores per Label:
HD: Precision=0.523 Recall=0.259 F1=0.346
CV: Precision=0.250 Recall=0.083 F1=0.125
VO: Precision=0.573 Recall=0.233 F1=0.331
NONE: Precision=0.907 Recall=0.968 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.882
Recall:    0.876
F1 Score:  0.878

Microaveraged Scores per Label:
HD: Precision=0.564 Recall=0.161 F1=0.250
CV: Precision=0.400 Recall=0.083 F1=0.138
VO: Precision=0.621 Recall=0.160 F1=0.254
NONE: Precision=0.895 Recall=0.982 F1=0.937

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.880
Recall:    0.877
F1 Score:  0.878

Microaveraged Scores per Label:
HD: Precision=0.638 Recall=0.090 F1=0.157
CV: Precision=0.500 Recall=0.083 F1=0.143
VO: Precision=0.706 Recall=0.098 F1=0.171
NONE: Precision=0.885 Recall=0.992 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.875
Recall:    0.874
F1 Score:  0.874

Microaveraged Scores per Label:
HD: Precision=0.600 Recall=0.031 F1=0.058
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.688 Recall=0.060 F1=0.110
NONE: Precision=0.877 Recall=0.996 F1=0.933

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_RandomUnderSampler_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.873
Recall:    0.872
F1 Score:  0.872

Microaveraged Scores per Label:
HD: Precision=0.667 Recall=0.008 F1=0.016
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.625 Recall=0.027 F1=0.052
NONE: Precision=0.874 Recall=0.998 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.411
Recall:    0.410
F1 Score:  0.407

Microaveraged Scores per Label:
HD: Precision=0.150 Recall=0.690 F1=0.246
CV: Precision=0.024 Recall=0.208 F1=0.043
VO: Precision=0.117 Recall=0.629 F1=0.197
NONE: Precision=0.986 Recall=0.372 F1=0.540

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.440
Recall:    0.433
F1 Score:  0.434

Microaveraged Scores per Label:
HD: Precision=0.157 Recall=0.633 F1=0.252
CV: Precision=0.022 Recall=0.167 F1=0.040
VO: Precision=0.126 Recall=0.585 F1=0.207
NONE: Precision=0.985 Recall=0.405 F1=0.574

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.472
Recall:    0.458
F1 Score:  0.463

Microaveraged Scores per Label:
HD: Precision=0.157 Recall=0.554 F1=0.244
CV: Precision=0.026 Recall=0.167 F1=0.045
VO: Precision=0.135 Recall=0.531 F1=0.215
NONE: Precision=0.982 Recall=0.444 F1=0.611

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.532
Recall:    0.520
F1 Score:  0.524

Microaveraged Scores per Label:
HD: Precision=0.163 Recall=0.499 F1=0.245
CV: Precision=0.022 Recall=0.125 F1=0.038
VO: Precision=0.137 Recall=0.450 F1=0.210
NONE: Precision=0.949 Recall=0.525 F1=0.676

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.596
Recall:    0.585
F1 Score:  0.589

Microaveraged Scores per Label:
HD: Precision=0.170 Recall=0.436 F1=0.245
CV: Precision=0.028 Recall=0.125 F1=0.045
VO: Precision=0.144 Recall=0.393 F1=0.211
NONE: Precision=0.931 Recall=0.608 F1=0.736

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTEENN_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.663
Recall:    0.654
F1 Score:  0.657

Microaveraged Scores per Label:
HD: Precision=0.173 Recall=0.344 F1=0.231
CV: Precision=0.032 Recall=0.125 F1=0.050
VO: Precision=0.157 Recall=0.320 F1=0.210
NONE: Precision=0.912 Recall=0.700 F1=0.792

Example Confidence Scores:
Test sample 0:
  HD: 0.300
  CV: 0.000
  VO: 0.025
  NONE: 0.675
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.760
  CV: 0.000
  VO: 0.236
  NONE: 0.004
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.033
  CV: 0.000
  VO: 0.003
  NONE: 0.964
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.712
  CV: 0.000
  VO: 0.002
  NONE: 0.286
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.916
  CV: 0.019
  VO: 0.065
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 5:
  HD: 0.869
  CV: 0.000
  VO: 0.131
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 6:
  HD: 0.094
  CV: 0.000
  VO: 0.139
  NONE: 0.767
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.232
  CV: 0.000
  VO: 0.756
  NONE: 0.012
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.146
  CV: 0.000
  VO: 0.270
  NONE: 0.584
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.865
  CV: 0.003
  VO: 0.122
  NONE: 0.010
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 10:
  HD: 0.994
  CV: 0.000
  VO: 0.006
  NONE: 0.000
  Ground truth: ['NONE']
  Predicted:    ['HD']

Test sample 11:
  HD: 0.029
  CV: 0.001
  VO: 0.970
  NONE: 0.001
  Ground truth: ['HD']
  Predicted:    ['VO']

Test sample 12:
  HD: 0.196
  CV: 0.000
  VO: 0.008
  NONE: 0.796
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.143
  CV: 0.093
  VO: 0.764
  NONE: 0.000
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTE_single_label.npy | Threshold: 0.3 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.3

Partial Evaluation Results:
Precision: 0.869
Recall:    0.865
F1 Score:  0.866

Microaveraged Scores per Label:
HD: Precision=0.454 Recall=0.371 F1=0.408
CV: Precision=0.250 Recall=0.125 F1=0.167
VO: Precision=0.490 Recall=0.339 F1=0.401
NONE: Precision=0.920 Recall=0.943 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['HD', 'VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTE_single_label.npy | Threshold: 0.4 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.4

Partial Evaluation Results:
Precision: 0.880
Recall:    0.874
F1 Score:  0.876

Microaveraged Scores per Label:
HD: Precision=0.523 Recall=0.259 F1=0.346
CV: Precision=0.250 Recall=0.083 F1=0.125
VO: Precision=0.573 Recall=0.233 F1=0.331
NONE: Precision=0.907 Recall=0.968 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['VO']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTE_single_label.npy | Threshold: 0.5 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.5

Partial Evaluation Results:
Precision: 0.882
Recall:    0.876
F1 Score:  0.878

Microaveraged Scores per Label:
HD: Precision=0.564 Recall=0.161 F1=0.250
CV: Precision=0.400 Recall=0.083 F1=0.138
VO: Precision=0.621 Recall=0.160 F1=0.254
NONE: Precision=0.895 Recall=0.982 F1=0.937

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['VO']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTE_single_label.npy | Threshold: 0.6 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.6

Partial Evaluation Results:
Precision: 0.880
Recall:    0.877
F1 Score:  0.878

Microaveraged Scores per Label:
HD: Precision=0.638 Recall=0.090 F1=0.157
CV: Precision=0.500 Recall=0.083 F1=0.143
VO: Precision=0.706 Recall=0.098 F1=0.171
NONE: Precision=0.885 Recall=0.992 F1=0.936

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTE_single_label.npy | Threshold: 0.7 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.7

Partial Evaluation Results:
Precision: 0.875
Recall:    0.874
F1 Score:  0.874

Microaveraged Scores per Label:
HD: Precision=0.600 Recall=0.031 F1=0.058
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.688 Recall=0.060 F1=0.110
NONE: Precision=0.877 Recall=0.996 F1=0.933

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.

== Training with: sing_label_data3_1/multiclass_SMOTE_single_label.npy | Threshold: 0.8 ==
Loading training data...
Loading test data...
Training model...
Evaluating with threshold: 0.8

Partial Evaluation Results:
Precision: 0.873
Recall:    0.872
F1 Score:  0.872

Microaveraged Scores per Label:
HD: Precision=0.667 Recall=0.008 F1=0.016
CV: Precision=0.000 Recall=0.000 F1=0.000
VO: Precision=0.625 Recall=0.027 F1=0.052
NONE: Precision=0.874 Recall=0.998 F1=0.932

Example Confidence Scores:
Test sample 0:
  HD: 0.026
  CV: 0.001
  VO: 0.007
  NONE: 0.966
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 1:
  HD: 0.063
  CV: 0.000
  VO: 0.031
  NONE: 0.906
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 2:
  HD: 0.080
  CV: 0.001
  VO: 0.051
  NONE: 0.868
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 3:
  HD: 0.005
  CV: 0.000
  VO: 0.001
  NONE: 0.994
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 4:
  HD: 0.123
  CV: 0.013
  VO: 0.036
  NONE: 0.828
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 5:
  HD: 0.226
  CV: 0.001
  VO: 0.424
  NONE: 0.349
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 6:
  HD: 0.013
  CV: 0.000
  VO: 0.005
  NONE: 0.982
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 7:
  HD: 0.007
  CV: 0.000
  VO: 0.009
  NONE: 0.983
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 8:
  HD: 0.005
  CV: 0.000
  VO: 0.004
  NONE: 0.991
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 9:
  HD: 0.054
  CV: 0.001
  VO: 0.013
  NONE: 0.932
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 10:
  HD: 0.013
  CV: 0.000
  VO: 0.002
  NONE: 0.984
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 11:
  HD: 0.077
  CV: 0.005
  VO: 0.222
  NONE: 0.696
  Ground truth: ['HD']
  Predicted:    ['NONE']

Test sample 12:
  HD: 0.027
  CV: 0.006
  VO: 0.005
  NONE: 0.962
  Ground truth: ['NONE']
  Predicted:    ['NONE']

Test sample 13:
  HD: 0.328
  CV: 0.018
  VO: 0.511
  NONE: 0.143
  Ground truth: ['HD']
  Predicted:    ['NONE']

Saved full confidence outputs to 'test_output_confidences.npy'.
